# Optimization_practice
Accuracy comparison using Gradient Descent with mini-batch, Momentum, and Adam.
Learning Rate Dacay & Scheduling included. 

# Used Package
Numpy, Matplotlib, Scipy, Sklearn, etc.

# Assessment
Higher accuracy, better result.
Result of this project are as follows.

# Result
Momentum 95.6% > Gradient Descent 94.6% > Adam 94%

# Personal Lesson
Gradient Descent is an effective tool, but it is not always the best solution for optimization. 
Better use another method if things are not going fine.
