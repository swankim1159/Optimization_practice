# Optimization_practice
Accuracy comparison using Gradient Descent with mini-batch, Momentum, and Adam.
Learning Rate Dacay and Scheduling included. 

# Used Packages.
Numpy, Matplotlib, Scipy, Sklearn, etc.

# Assessment
Higher accuracy, better result.
The results of this project are as follows.

# Result
Momentum 95.6% > Gradient Descent 94.6% > Adam 94%

# Personal Lesson
Gradient Descent is an effective tool, but it is not always the best solution for optimization. 
Consider other optimization methods if results are unsatisfactory.
